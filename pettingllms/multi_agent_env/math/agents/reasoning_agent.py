import copy
import logging
from typing import Any
from pettingllms.multi_agent_env.base.agent import Agent, AgentData
from pettingllms.multi_agent_env.base.env import Env
from typing import List
from math_verify import parse, verify
logger = logging.getLogger(__name__)


def truncatefn(s, length=300):
    if isinstance(s, str):
        pass
    else:
        s = str(s)
    if len(s) <= length:
        return s

    return s[:500] + "\n\n...(reasoning steps truncated)...\n\n" + s[-500:]


class ReasoningAgent(Agent):
    """
    Agent specialized for solving mathematical problems.
    """

    def __init__(self, rollout_idx: int | None = None, **kwargs):
        """
        Initialize the Math Solving Agent's data.
        """
        super().__init__()
        self.rollout_idx = rollout_idx
        for key, value in (kwargs or {}).items():
            setattr(self, key, value)

    def update_from_env(self, turn_idx: int, env_data: Env):
        # Save environment data
        self.env_data = env_data

        # Support passing either the raw environment (with state) or a wrapped Env
        state = getattr(env_data, "state", None)
        

        problem = getattr(state, "problem", None)
        reasoning_generated_solution_history = getattr(state, "reasoning_generated_solution_history", None)
        reasoning_extracted_answer_history = getattr(state, "reasoning_extracted_answer_history", None)
        code_generated_solution_history = getattr(state, "code_generated_solution_history", None)
        code_extracted_answer_history = getattr(state, "code_extracted_answer_history", None)
        prompt_for_history="Here is the history of previous reasoning and code solutions generated by LLMs:\n"
        if reasoning_generated_solution_history is not None:
            for i in range(len(reasoning_generated_solution_history)):
                prompt_for_history += f"The turn {i+1} reasoning solution is {reasoning_generated_solution_history[i]}\n"
                prompt_for_history += f"The turn {i+1}th reasoning extracted answer is {reasoning_extracted_answer_history[i]}\n"
        
        if code_generated_solution_history is not None:
            for i in range(len(code_generated_solution_history)):
                prompt_for_history += f"The turn {i+1} code solution is {code_generated_solution_history[i]}\n"
                prompt_for_history += f"The turn {i+1}th code execution result is {code_extracted_answer_history[i]}\n"
        
        if turn_idx == 0:
           formatted_prompt = (
        f"Problem:\n{problem}\n\n"
        f"Please think step by step and output the final answer in \\boxed{{}} format.\n"
        f"Example: \\boxed{{123}}\n\n"
    )
        else:
            formatted_prompt = (
                f"You are a helpful assistant that select or refines the best mathematical solutions through reasoning.\n\n"
                f"Problem:\n{problem}\n\n")
            formatted_prompt += prompt_for_history
            formatted_prompt += (
                f"Please firstly select or refine the best solution from the history of solutions.\n"
                f"The previous solutions are possible to be correct and possible to be all incorrect. If one of the solutions is correct, select it. If all the solutions are incorrect, refine the best solution.\n"
                f"Then solve the problem again. Over all, if the computation is too comlex, the reasoning might be more easier to make mistake.\n")
            formatted_prompt += (
               f"Before giving the full reasoning, please summarize the key reasoning steps clearly.\n"
                f"Output them in the following format:\n\n"
                f"**Reasoning Steps:**\n```reasoning steps here```\n\n"
                f"Put your final answer in the format of \\boxed{{<answer>}}\n"
                f"For the final answer, only output the answer after \\boxed{{}}, no other text.\n"
                f"Example: \\boxed{{123}}\n\n"
            )
        
        self.current_prompt = {"text": formatted_prompt, "image": None}
        
    
    def update_from_model(self, response: str):
        
        self.current_action = response
        return self.current_action

    async def step(self, env_data: Env, env_worker: Any = None):
        """
        Process the generated reasoning solution and evaluate it against the ground truth.
        """
       
       
        env_data.state.reasoning_generated_solution = truncatefn(self.current_action)
        env_data.state.reasoning_extracted_answer = parse(self.current_action)
        env_data.state.reasoning_generated_solution_history.append(env_data.state.reasoning_generated_solution)
        env_data.state.reasoning_extracted_answer_history.append(env_data.state.reasoning_extracted_answer)
        self.answer_history.append(env_data.state.reasoning_extracted_answer)
        self.action_history.append(self.current_action)
        extracted_answer = env_data.state.reasoning_extracted_answer
        ground_truth_answer = env_data.state.ground_truth_answer
        is_correct = False
        
        if extracted_answer is not None and ground_truth_answer is not None:
            
            is_correct =verify(extracted_answer, parse(ground_truth_answer))
            env_data.state.reasoning_is_correct = is_correct
            
            if is_correct:
                self.success = True
                env_data.state.reasoning_is_correct = True
            else:
                self.success = False
                env_data.state.reasoning_is_correct = False
        
        if env_data.state.code_extracted_answer is not None and env_data.state.reasoning_extracted_answer is not None:
            is_aligned = verify(env_data.state.code_extracted_answer, env_data.state.reasoning_extracted_answer)
            env_data.state.code_reasoning_aligned = bool(is_aligned)
            if is_aligned:
                env_data.state.code_reasoning_aligned = True
                env_data.done = True

        
    
    def calculate_reward(self, env_data: Env):
        self.agent_reward = int(env_data.state.reasoning_is_correct)
        self.reward_history.append(self.agent_reward)

    def reset(self):
        """
        Reset the agent's internal state for a new episode.
        """
        self.current_action = None
        self.current_prompt = None
        self.current_response = None
        self.current_reward = None
        self.current_info = None
        self.current_action = None
        self.current_prompt = None
        self.current_response = None
