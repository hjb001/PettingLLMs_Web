import copy
import logging
from typing import Any, List
from pettingllms.multi_agent_env.base.agent import Agent, AgentData
from pettingllms.multi_agent_env.base.env import Env
from pettingllms.multi_agent_env.math.math_utils import extract_code
from pettingllms.multi_agent_env.math.math_worker import get_code_execution_output
from math_verify import parse, verify
logger = logging.getLogger(__name__)


def truncatefn(s, length=300):
    if isinstance(s, str):
        pass
    else:
        s = str(s)
    if len(s) <= length:
        return s

    return s[: length // 2] + "...(truncated) ..." + s[-length // 2 :]


class SampleToolAgent(Agent):
    """
    Agent specialized for solving mathematical problems using code with multiple sampling support.
    Can generate sample_num different solutions per step.
    """

    def __init__(self, rollout_idx: int | None = None, **kwargs):
        """
        Initialize the Sampling Tool Agent's data.
        """
        super().__init__()
        self.rollout_idx = rollout_idx
        self.sample_num = kwargs.get('sample_num', 1)  # Number of samples to generate
        # Accept other unrelated keyword arguments for compatibility
        for key, value in (kwargs or {}).items():
            setattr(self, key, value)

    def update_from_env(self, turn_idx: int, env_data: Env):
        # Save environment data
        self.env_data = env_data

        # Support passing either the raw environment (with state) or a wrapped Env
        state = getattr(env_data, "state", None)

        problem = getattr(state, "problem", None)
        code_generated_solution_history = getattr(state, "code_generated_solution_history", None)
        code_extracted_answer_history = getattr(state, "code_extracted_answer_history", None)
        reasoning_generated_solution_history = getattr(state, "reasoning_generated_solution_history", None)
        reasoning_extracted_answer_history = getattr(state, "reasoning_extracted_answer_history", None)
        
        prompt_for_history = "Here is the history of previous reasoning and code solutions generated by LLMs:\n"
        if reasoning_generated_solution_history is not None and len(reasoning_generated_solution_history) > 0:
            for i in range(len(reasoning_generated_solution_history)):
                prompt_for_history += f"The turn {i+1} reasoning solution is {truncatefn(reasoning_generated_solution_history[i], 500)}\n"
                prompt_for_history += f"The turn {i+1}th reasoning extracted answer is {reasoning_extracted_answer_history[i]}\n"
        
        if code_generated_solution_history is not None and len(code_generated_solution_history) > 0:
            for i in range(len(code_generated_solution_history)):
                prompt_for_history += f"The turn {i+1} code solution is {truncatefn(code_generated_solution_history[i], 500)}\n"
                prompt_for_history += f"The turn {i+1}th code execution result is {code_extracted_answer_history[i]}\n"

        if turn_idx == 0:
            formatted_prompt = (
                f"You are a helpful programming assistant that write python code to solve mathematical problems through step-by-step reasoning.\n\n"
                f"Problem:\n{problem}\n\n"
                f"You need to think step by step and provide a complete solution using python code with clear mathematical reasoning.\n"
                f"Please write Python code to solve this problem.\n And you need to print the final answer in the code. Like if the final anwer is the variable x, you need to write ```print(x)```.\n"
                f"Respond in the format:\n\n"
                f"**Code:**\n```python\n# corrected code here\n```\n\n" 
            )
        else:
            formatted_prompt = (
                f"You are a helpful programming assistant that write python code to solve mathematical problems through step-by-step reasoning. \n\n"
                f"Problem:\n{problem}\n\n"
            )
            formatted_prompt += prompt_for_history
            formatted_prompt += (
                f"Please firstly select or refine the best solution from the history of solutions.\n"
                f"The previous solutions are possible to be correct and possible to be all incorrect. If one of the solutions is correct, select it. If all the solutions are incorrect, refine the best solution.\n"
                f"Then solve the problem again. Over all, if the computation is too comlex, the reasoning might be more easier to make mistake. I want you to write the code to solve the problem again.\n"
            )
            
            formatted_prompt += (
                f"Respond in the format:\n\n"
                f"**Code:**\n```python\n# corrected code here\n```\n\n" 
            )
        
        self.current_prompt = {"text": formatted_prompt, "image": None}
        
    def update_from_model(self, response: str):
        """
        Parse the response and update agent_data. Can handle single or multiple responses.
        """
        self.current_action = extract_code(response)
        return self.current_action

    async def step(self, env_data: Env, env_worker: Any = None):
        """
        Process the generated code solution and evaluate it against the ground truth.
        """
        generated_solution = self.current_action
        env_data.state.code_generated_solution = generated_solution
        ground_truth_answer = env_data.state.ground_truth_answer
        is_correct = False
        code_execution_output = None
        
        try:
            # execute the code (through ray worker)
            code_execution_output = await get_code_execution_output(
                generated_solution,
                timeout=20.0,
                ray_actor=env_worker,
            )
            env_data.state.code_extracted_answer = parse(code_execution_output)
            # Update history records
            env_data.state.code_generated_solution_history.append(env_data.state.code_generated_solution)
            env_data.state.code_extracted_answer_history.append(env_data.state.code_extracted_answer)
            self.answer_history.append(env_data.state.code_extracted_answer)
            self.action_history.append(self.current_action)
            
            if code_execution_output is None:
                self.agent_reward = -1
                return
            
            is_correct = verify(parse(code_execution_output), parse(ground_truth_answer))
            if is_correct:
                self.success = True
                self.agent_reward = 1.0
                env_data.state.code_is_correct = True
            else:
                self.success = False
                self.agent_reward = 0.0
                env_data.state.code_is_correct = False
        except Exception as e:
            code_execution_output = f"error: {e}"
            env_data.state.code_extracted_answer = code_execution_output
            self.agent_reward = -1

    def calculate_reward(self, env_data: Env):
        """
        Calculate reward based on code execution correctness.
        """
        self.agent_reward = self.agent_reward
        self.reward_history.append(self.agent_reward)
 
    def reset(self):
        """
        Reset the agent's internal state for a new episode.
        """
        self.current_action = None
        self.current_prompt = None
        self.current_response = None
        self.current_reward = None
        self.current_info = None
        self.current_action = None
        self.current_prompt = None
        self.current_response = None
